# -*- coding: utf-8 -*-
"""Loan Forecasting using PCA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w5e3GE1AhnRpEkSuxMXgtjyHjwWYCZTh
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
!pip install tensorflow
!pip install -U scikit-learn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils import resample


from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout,BatchNormalization

from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam

import os
import random
from tensorflow.random import set_seed

from sklearn.decomposition import PCA

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/1_data/kiva_loans.csv'
train_df=pd.read_csv(file_path)
print(train_df.head())

print(train_df.columns)

train_df.shape

import matplotlib.pyplot as plt

sns.boxplot(x='funded_state', y='loan_amount', data=train_df)
plt.title('Correlation between Loan Amount and Funded State')
plt.xlabel('Funded State')
plt.ylabel('Loan Amount')
plt.show()

train_df_check = train_df[
    [
        'posted_time',
        'funded_state',
    ]
]

train_df_check['posted_time'] = pd.to_datetime(train_df_check['posted_time'])

train_df_check['year_month'] = train_df_check['posted_time'].dt.to_period('M')

monthly_probabilities = train_df_check.groupby('year_month')['funded_state'].mean()

plt.figure(figsize=(10, 6))
monthly_probabilities.plot(kind='line', marker='o')
plt.title('Monthly Probability of funded_state = 1')
plt.xlabel('Month')
plt.ylabel('Probability')
plt.grid(True)
plt.show()

train_df_check = train_df[
    [
        'sector',
        'funded_state',
    ]
]

train_df_check = pd.get_dummies(train_df_check, drop_first=True)

for sector in train_df_check.columns[:-1]:
    total_in_sector = train_df_check[sector].sum()
    funded_in_sector = train_df_check[(train_df_check[sector] == 1) & (train_df_check['funded_state'] == 1)].shape[0]
    if total_in_sector > 0:
        probability = funded_in_sector / total_in_sector
        print(f"Probability of funding for {sector}: {probability:.2f}")
    else:
        print(f"No loans in {sector}")

train_df_check = train_df[
    [
        'country',
        'funded_state',
    ]
]

train_df_check = pd.get_dummies(train_df_check, drop_first=True)
for sector in train_df_check.columns[:-1]:
    total_in_sector = train_df_check[sector].sum()
    funded_in_sector = train_df_check[(train_df_check[sector] == 1) & (train_df_check['funded_state'] == 1)].shape[0]
    if total_in_sector > 0:
        probability = funded_in_sector / total_in_sector
        print(f"Probability of funding for {sector}: {probability:.2f}")
    else:
        print(f"No loans in {sector}")

train_df_selected_origin = train_df[
    [
        'repayment_interval',
        'sector',
        'country',
        'funded_state',
        'posted_time',
    ]
]

train_df_selected_origin['year_month'] = pd.to_datetime(train_df_selected_origin['posted_time']).dt.to_period('M')

train_df_selected_origin = pd.get_dummies(train_df_selected_origin, columns=['year_month'], drop_first=True)

train_df_selected_origin = train_df_selected_origin.drop('posted_time', axis=1)

train_df_selected = pd.get_dummies(train_df_selected_origin, drop_first=True)

train_df_selected.info()
print(train_df_selected.columns)

X = train_df_selected.drop(columns=['funded_state'])
y = train_df_selected['funded_state']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("X_train:", X_train.shape)
print("X_test:", X_test.shape)
print("y_train:", y_train.shape)
print("y_test:", y_test.shape)

def set_all_seeds(seed=42):
    # tensorflow/kerasの乱数を固定
    set_seed(seed)
    # NumPyの乱数を固定
    np.random.seed(seed)
    # Pythonの標準ライブラリの乱数を固定
    random.seed(seed)
    # ハッシュ関数のシードを固定
    os.environ["PYTHONHASHSEED"] = str(seed)

# すべての乱数を固定
set_all_seeds(42)

pca = PCA(n_components=0.9)

pca.fit(X_train)

# 元のデータを主成分得点に変換
X_train_pca = pca.transform(X_train)
print(X_train_pca.shape)

# テスト用データにも同様の変換を行う
X_test_pca = pca.transform(X_test)
print(X_test_pca.shape)

print(X_train_pca.shape[1])

model = Sequential()

model.add(Dense(64,input_dim=X_train_pca.shape[1], activation='swish'))
model.add(BatchNormalization())

model.add(Dense(32, activation='swish'))
model.add(BatchNormalization())

model.add(Dense(16, activation='swish'))
model.add(BatchNormalization())

model.add(Dropout(0.5))

model.add(Dense(1, activation='sigmoid'))
adam = Adam(learning_rate=0.001)

model.compile(
    loss='binary_crossentropy',
    optimizer=adam,
    metrics=['accuracy']
)

model.summary()

start_time = time.time()
fit = model.fit(
    X_train_pca,
    y_train,
    epochs=75,
    batch_size=40,
    validation_data=(X_test_pca,y_test),
    verbose=1
)
end_time = time.time()
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

df=pd.DataFrame(fit.history)

# Loss(損失) をグラフ化
plt.figure(figsize=(12,8))
sns.lineplot(data=df[["loss","val_loss"]])
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(loc='upper right')
plt.show()

# accuracy(正解率)をグラフ化
plt.figure(figsize=(12,8))
sns.lineplot(data=df[["accuracy","val_accuracy"]])
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.show()

weights=model.get_weights()

for i in range(len(weights)):
  print(f"weights[{i}]=")
  print(weights[i])
  print("num:",weights[i].flatten().shape[0])
  print()

loss, accuracy = model.evaluate(X_test_pca, y_test, verbose=0)
print(f'Accuracy: {accuracy}')