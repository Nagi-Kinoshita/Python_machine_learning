# -*- coding: utf-8 -*-
"""Loan Forecasting using some models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3-Okl0fYd-j_aHNMBTX5IwacgozGTY_
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
!pip install tensorflow
!pip install -U scikit-learn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils import resample


from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout,BatchNormalization

from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam

import os
import random
from tensorflow.random import set_seed

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/1_data/kiva_loans.csv'
train_df=pd.read_csv(file_path)
print(train_df.head())

print(train_df.columns)

train_df.shape

import matplotlib.pyplot as plt

sns.boxplot(x='funded_state', y='loan_amount', data=train_df)
plt.title('Correlation between Loan Amount and Funded State')
plt.xlabel('Funded State')
plt.ylabel('Loan Amount')
plt.show()

train_df_check = train_df[
    [
        'posted_time',
        'funded_state',
    ]
]

train_df_check['posted_time'] = pd.to_datetime(train_df_check['posted_time'])

train_df_check['year_month'] = train_df_check['posted_time'].dt.to_period('M')

monthly_probabilities = train_df_check.groupby('year_month')['funded_state'].mean()

plt.figure(figsize=(10, 6))
monthly_probabilities.plot(kind='line', marker='o')
plt.title('Monthly Probability of funded_state = 1')
plt.xlabel('Month')
plt.ylabel('Probability')
plt.grid(True)
plt.show()

train_df_check = train_df[
    [
        'sector',
        'funded_state',
    ]
]

train_df_check = pd.get_dummies(train_df_check, drop_first=True)

for sector in train_df_check.columns[:-1]:
    total_in_sector = train_df_check[sector].sum()
    funded_in_sector = train_df_check[(train_df_check[sector] == 1) & (train_df_check['funded_state'] == 1)].shape[0]
    if total_in_sector > 0:
        probability = funded_in_sector / total_in_sector
        print(f"Probability of funding for {sector}: {probability:.2f}")
    else:
        print(f"No loans in {sector}")

train_df_check = train_df[
    [
        'country',
        'funded_state',
    ]
]

train_df_check = pd.get_dummies(train_df_check, drop_first=True)
for sector in train_df_check.columns[:-1]:
    total_in_sector = train_df_check[sector].sum()
    funded_in_sector = train_df_check[(train_df_check[sector] == 1) & (train_df_check['funded_state'] == 1)].shape[0]
    if total_in_sector > 0:
        probability = funded_in_sector / total_in_sector
        print(f"Probability of funding for {sector}: {probability:.2f}")
    else:
        print(f"No loans in {sector}")

train_df_selected_origin = train_df[
    [
        'repayment_interval',
        'sector',
        'country',
        'funded_state',
        'posted_time',
    ]
]

train_df_selected_origin['year_month'] = pd.to_datetime(train_df_selected_origin['posted_time']).dt.to_period('M')

train_df_selected_origin = pd.get_dummies(train_df_selected_origin, columns=['year_month'], drop_first=True)

train_df_selected_origin = train_df_selected_origin.drop('posted_time', axis=1)

train_df_selected = pd.get_dummies(train_df_selected_origin, drop_first=True)

train_df_selected.info()
print(train_df_selected.columns)

X = train_df_selected.drop(columns=['funded_state'])
y = train_df_selected['funded_state']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("X_train:", X_train.shape)
print("X_test:", X_test.shape)
print("y_train:", y_train.shape)
print("y_test:", y_test.shape)

def set_all_seeds(seed=42):
    # tensorflow/kerasの乱数を固定
    set_seed(seed)
    # NumPyの乱数を固定
    np.random.seed(seed)
    # Pythonの標準ライブラリの乱数を固定
    random.seed(seed)
    # ハッシュ関数のシードを固定
    os.environ["PYTHONHASHSEED"] = str(seed)

# すべての乱数を固定
set_all_seeds(42)

from sklearn.tree import DecisionTreeClassifier

# 決定木モデルの構築
model = DecisionTreeClassifier(criterion="gini",max_depth=None,min_samples_split=3,min_samples_leaf=3)

start_time = time.time()  # 開始時間を記録
model.fit(X_train, y_train)  # モデルの訓練
end_time = time.time()  # 終了時間を記録

# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# テストデータを用いて予測
y_pred = model.predict(X_test)

# 精度を計算して表示
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from sklearn.ensemble import RandomForestClassifier

# ランダムフォレスト分類モデルの構築
model = RandomForestClassifier(n_estimators=10, max_depth=2, criterion="gini",min_samples_split=2,min_samples_leaf=2)

# モデルの訓練
start_time = time.time()  # 開始時間を記録
model.fit(X_train, y_train)  # モデルの訓練
end_time = time.time()  # 終了時間を記録

# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# テストデータで予測を行う
y_pred = model.predict(X_test)

# 精度を計算して表示
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from sklearn.ensemble import AdaBoostClassifier

# AdaBoost分類モデルの構築
model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3,criterion="gini",min_samples_split=2,min_samples_leaf=2),n_estimators=10)

# モデルの訓練
start_time = time.time()  # 開始時間を記録
model.fit(X_train, y_train)  # モデルの訓練
end_time = time.time()  # 終了時間を記録

# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# テストデータで予測を行う
y_pred = model.predict(X_test)

# 精度を計算して表示
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from xgboost import XGBClassifier

# AdaBoost分類モデルの構築
model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=10, objective="binary:logistic")

# モデルの訓練
start_time = time.time()  # 開始時間を記録
model.fit(X_train, y_train)  # モデルの訓練
end_time = time.time()  # 終了時間を記録

# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# テストデータで予測を行う
y_pred = model.predict(X_test)

# 精度を計算して表示
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from sklearn.svm import SVC

# サポートベクターマシン分類モデルの構築
model = SVC(C=1,kernel='linear',gamma=1)

start_time = time.time()  # 開始時間を記録
model.fit(X_train, y_train)  # モデルの訓練
end_time = time.time()  # 終了時間を記録

# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# テストデータで予測を行う
y_pred = model.predict(X_test)

# 精度を計算して表示
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print(X_train.shape[1])

def create_base_model(input_dim):
    model = Sequential()

    # 入力層と第1の隠れ層
    model.add(Dense(141, input_dim=input_dim, activation='swish'))
    model.add(BatchNormalization())

    # 第2の隠れ層
    model.add(Dense(70, activation='swish'))
    model.add(BatchNormalization())

    # 第3の隠れ層
    model.add(Dense(40, activation='swish'))
    model.add(BatchNormalization())

    # ドロップアウト
    model.add(Dropout(0.5))

    # 出力層
    model.add(Dense(1, activation='sigmoid'))

    # 最適化手法の設定
    adam = Adam(learning_rate=0.001)

    # モデルの学習プロセスを設定
    model.compile(
        loss='binary_crossentropy',
        optimizer=adam,
        metrics=['accuracy']
    )

    return model

# モデルのリスト
models = []

start_time = time.time()  # 開始時間を記録
# バギングのための複数のモデルをトレーニング
for i in range(5):  # 5つのモデルを作成
    model = create_base_model(X_train.shape[1])  # ベースとなるニューラルネットワークを定義
    X_resampled, y_resampled = resample(X_train, y_train)  # データのリサンプリング
    model.fit(X_resampled, y_resampled, epochs=10, verbose=0,batch_size=32)
    models.append(model)

end_time = time.time()  # 終了時間を記録
# 学習時間を計算
training_time = end_time - start_time
print(f"Model training time: {training_time:.2f} seconds")

# アンサンブル予測
predictions = np.mean([model.predict(X_test) for model in models], axis=0)
final_predictions = (predictions > 0.5).astype(int)  # 二値分類の場合

accuracy = accuracy_score(y_test, final_predictions)

print(f"Accuracy: {accuracy}")